{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRIDGE: M9.3 HyDE → M9.4 Advanced Reranking\n",
    "\n",
    "## Purpose\n",
    "\n",
    "**What shifts:** You've mastered vocabulary-mismatch retrieval with HyDE (M9.3), achieving 58% precision improvement by generating hypothetical documents. Now you face a new problem—your hybrid retrieval returns the **right documents but in the wrong order**. M9.4 introduces advanced reranking to optimize ranking with recency, diversity, and personalization signals without violating latency SLAs.\n",
    "\n",
    "**Why it matters:** 20-30% of production queries with temporal or diverse content suffer from poor ranking. A 2023 CPRA article buried at rank #48 below outdated 2019 GDPR articles costs teams 33 hours/day in manual re-sorting (₹495K/month lost productivity). Advanced reranking solves ordering problems that retrieval quality alone cannot fix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts Covered\n",
    "\n",
    "**Delta from M9.3:**\n",
    "- Validating hypothesis generator quality (80%+ domain-appropriate)\n",
    "- Measuring hybrid retriever precision on vocabulary-mismatch queries (P@10 ≥75%)\n",
    "- Testing query classifier routing accuracy (85%+ factoid detection)\n",
    "- Verifying semantic cache performance (30%+ hit rate, ≤400ms P95 latency)\n",
    "\n",
    "**Preview of M9.4:**\n",
    "- Ensemble cross-encoder systems with voting\n",
    "- MMR (Maximal Marginal Relevance) for diversity\n",
    "- Temporal boosting with exponential decay\n",
    "- Personalized ranking via CTR-based signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Completing This Bridge\n",
    "\n",
    "You will be able to:\n",
    "- ✓ Verify your M9.3 HyDE implementation meets minimum quality thresholds for advanced reranking\n",
    "- ✓ Identify temporal and diversity issues in your top-K retrieval results\n",
    "- ✓ Assess whether your system needs advanced reranking (or if a single cross-encoder suffices)\n",
    "- ✓ Understand the four readiness criteria that prevent reranking failures in M9.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context in Track\n",
    "\n",
    "**Bridge:** Level 3, Module 9.3 → Module 9.4  \n",
    "**Previous:** M9.3 Hypothetical Document Embeddings (HyDE)  \n",
    "**Next:** M9.4 Advanced Reranking Strategies  \n",
    "**Module:** Module 9 - Advanced Retrieval Techniques  \n",
    "**Duration:** 8-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run Locally\n",
    "\n",
    "**Windows:**\n",
    "```powershell\n",
    "powershell -c \"$env:PYTHONPATH='$PWD'; jupyter notebook\"\n",
    "```\n",
    "\n",
    "**Linux/macOS:**\n",
    "```bash\n",
    "PYTHONPATH=$PWD jupyter notebook\n",
    "```\n",
    "\n",
    "**Note:** This notebook runs offline-friendly. External service checks (LLM APIs, datasets, cache logs) gracefully skip if unavailable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: RECAP - What M9.3 HyDE Shipped\n",
    "\n",
    "### Achievements from M9.3 Hypothetical Document Embeddings:\n",
    "\n",
    "✓ **LLM-powered hypothesis generator**  \n",
    "Built system that transforms user queries into document-style answers, achieving 80%+ domain-appropriate hypotheses with GPT-4o-mini and domain context prompts\n",
    "\n",
    "✓ **Hybrid retriever with RRF fusion**  \n",
    "Implemented Reciprocal Rank Fusion combining HyDE and traditional retrieval, improving precision 15-40% on vocabulary-mismatch queries while maintaining performance on well-phrased queries\n",
    "\n",
    "✓ **Query classifier with adaptive routing**  \n",
    "Created factoid detection and vocabulary overlap checker (85%+ accuracy), routing only 30-40% of queries to HyDE to avoid latency/cost overhead\n",
    "\n",
    "✓ **Semantic cache achieving 30-40% hit rate**  \n",
    "Reduced effective latency from 500ms to 325ms by caching hypotheses for similar queries using embedding similarity threshold\n",
    "\n",
    "### Key Outcome:\n",
    "Precision improved 58% for queries where users and documents speak different languages (vocabulary mismatch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Readiness Check #1 - Hypothesis Generator\n",
    "\n",
    "**Requirement:** Hypothesis generator achieving 80%+ domain-appropriate hypotheses\n",
    "\n",
    "**Test:** Evaluate 20 queries to verify hypotheses match document style\n",
    "\n",
    "**Impact:** Advanced reranking builds on retrieval quality—poor hypotheses = poor retrieved docs = nothing to rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 1: LLM API Availability\n",
    "\n",
    "Verify LLM API keys are configured for hypothesis generation. If unavailable, this check skips gracefully (offline-friendly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readiness Check #1: Hypothesis Generator Quality\n",
    "import os\n",
    "\n",
    "# Skip guard: Check if LLM API key is available\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") or os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"⚠️ Skipping (no LLM API keys)\")\n",
    "else:\n",
    "    print(\"✓ LLM API configured\")\n",
    "    # Expected: Test 20 queries, score hypothesis quality\n",
    "    # Expected: Calculate: (domain_appropriate_count / 20) >= 0.80\n",
    "    print(\"# Expected: 16+/20 hypotheses match document style (80%+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Readiness Check #2 - Hybrid Retriever Precision\n",
    "\n",
    "**Requirement:** Hybrid retriever precision ≥75% on vocabulary-mismatch queries\n",
    "\n",
    "**Test:** Run offline eval on 50 mismatch queries, measure P@10 precision\n",
    "\n",
    "**Impact:** Reranking can only fix ordering, not fix fundamentally poor retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 2: Evaluation Dataset Availability\n",
    "\n",
    "Look for evaluation dataset with vocabulary-mismatch queries and ground truth labels. Skips if dataset is not present (offline-friendly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readiness Check #2: Hybrid Retriever Precision\n",
    "import os\n",
    "\n",
    "# Skip guard: Check for evaluation dataset\n",
    "eval_dataset_path = \"eval_mismatch_queries.json\"\n",
    "\n",
    "if not os.path.exists(eval_dataset_path):\n",
    "    print(\"⚠️ Skipping (no eval dataset at eval_mismatch_queries.json)\")\n",
    "else:\n",
    "    print(f\"✓ Eval dataset found: {eval_dataset_path}\")\n",
    "    # Expected: Load 50 vocab-mismatch queries with ground truth\n",
    "    # Expected: Run hybrid retrieval (HyDE + traditional + RRF)\n",
    "    # Expected: Calculate P@10 = relevant_in_top10 / total_queries >= 0.75\n",
    "    print(\"# Expected: P@10 >= 0.75 (37.5+/50 queries)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Readiness Check #3 - Query Classifier Routing\n",
    "\n",
    "**Requirement:** Query classifier routing correctly (85%+ factoid detection accuracy)\n",
    "\n",
    "**Test:** Test 40 queries (20 factoid, 20 conceptual), verify routing decisions\n",
    "\n",
    "**Impact:** Temporal reranking hurts factoid queries (e.g., \"When did X happen?\")—need correct routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 3: Query Classifier Accuracy\n",
    "\n",
    "Test query classifier with sample factoid and conceptual queries to verify routing accuracy meets threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readiness Check #3: Query Classifier Routing Accuracy\n",
    "\n",
    "# Sample test set (offline-friendly: no external calls)\n",
    "test_queries = {\n",
    "    \"factoid\": [\"When did GDPR pass?\", \"Who invented Python?\"],\n",
    "    \"conceptual\": [\"What are data privacy regulations?\", \"How does machine learning work?\"]\n",
    "}\n",
    "\n",
    "print(\"✓ Query classifier test set loaded\")\n",
    "# Expected: Test 40 queries (20 factoid, 20 conceptual)\n",
    "# Expected: Classify each query, compare with ground truth\n",
    "# Expected: Accuracy = correct_classifications / 40 >= 0.85\n",
    "print(\"# Expected: 34+/40 queries correctly classified (85%+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Readiness Check #4 - Semantic Cache Performance\n",
    "\n",
    "**Requirement:** Semantic cache achieving 30%+ hit rate (latency ≤400ms P95)\n",
    "\n",
    "**Test:** Monitor cache metrics in logs, verify P95 latency under 400ms\n",
    "\n",
    "**Impact:** Advanced reranking adds +100-200ms—if base latency already high, total becomes unacceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 4: Cache Metrics Availability\n",
    "\n",
    "Parse cache metrics from logs to calculate hit rate and P95 latency. Skips gracefully if logs are unavailable (offline-friendly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readiness Check #4: Semantic Cache Hit Rate and Latency\n",
    "import os\n",
    "\n",
    "# Skip guard: Check for cache metrics log\n",
    "cache_log_path = \"cache_metrics.log\"\n",
    "\n",
    "if not os.path.exists(cache_log_path):\n",
    "    print(\"⚠️ Skipping (no cache metrics at cache_metrics.log)\")\n",
    "else:\n",
    "    print(f\"✓ Cache log found: {cache_log_path}\")\n",
    "    # Expected: Parse cache hits/misses\n",
    "    # Expected: Calculate hit_rate = hits / (hits + misses) >= 0.30\n",
    "    # Expected: Parse latency samples, calculate P95 <= 400ms\n",
    "    print(\"# Expected: Hit rate >= 30% AND P95 latency <= 400ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: CALL-FORWARD - What M9.4 Advanced Reranking Will Introduce\n",
    "\n",
    "### The Problem Advanced Reranking Solves:\n",
    "\n",
    "Your hybrid retrieval (decomposition + multi-hop + HyDE) returns the **right documents**—but in the **wrong order**.\n",
    "\n",
    "**Example Issues:**\n",
    "- **Recency:** 2019 GDPR article ranked #1, but 2023 CPRA article (more relevant) buried at #48\n",
    "- **Diversity:** Top-5 results all from same source (redundant, not diverse perspectives)\n",
    "- **Performance:** Ensemble rerankers add 300-600ms latency, violating SLAs\n",
    "\n",
    "### M9.4 Advanced Reranking Strategies Will Cover:\n",
    "\n",
    "**1. Ensemble Cross-Encoder Systems with Voting**\n",
    "- Combine 2-3 cross-encoder models with different strengths\n",
    "- Use majority voting or weighted averaging\n",
    "- Improve accuracy 8-12% while optimizing latency <200ms P95\n",
    "\n",
    "**2. MMR (Maximal Marginal Relevance) for Diversity**\n",
    "- Balance relevance vs diversity using MMR algorithm\n",
    "- Parameter λ controls trade-off (λ=1: pure relevance, λ=0: pure diversity)\n",
    "- Ensure top-5 results have 3+ unique sources\n",
    "\n",
    "**3. Temporal Boosting & Personalized Ranking**\n",
    "- Apply recency scoring with exponential decay\n",
    "- Learn user preferences from click data (CTR-based personalization)\n",
    "- Combine 4 signals: relevance + recency + diversity + personalization\n",
    "\n",
    "### Key Question for M9.4:\n",
    "\n",
    "**\"Your retrieval returns the right documents. But how do you rank them optimally considering recency, diversity, and user preferences—without adding 500ms latency?\"**\n",
    "\n",
    "### Reality Check:\n",
    "\n",
    "80-90% of use cases **don't need** advanced reranking. A single cross-encoder is sufficient if your content is:\n",
    "- Evergreen (doesn't change over time)\n",
    "- Naturally diverse (no redundancy in top results)\n",
    "- Used by anonymous users (no personalization needed)\n",
    "\n",
    "Advanced reranking is specifically for:\n",
    "- News/regulatory content (recency critical)\n",
    "- Research/analysis (diversity valuable)\n",
    "- Personalized systems (user preferences matter)\n",
    "\n",
    "---\n",
    "\n",
    "**Bridge Complete! Ready for M9.4 Advanced Reranking Strategies.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
